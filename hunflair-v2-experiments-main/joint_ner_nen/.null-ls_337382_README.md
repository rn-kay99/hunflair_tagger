# Cross-corpus evaluation on joint NER and NEN 

This corresponds to the section TODO in the paper.
As there are no corpora with entity mentions normalized to CTD vocabularies
which were not used to train the tools considered in the evaluation.
For this reason we resort to use two other corpora, MedMentions and CRAFT,
by mapping their annotations to identifiers in the CTD vocabularies.

## UMLS

You need to obtain the UMLS version used by MedMentions: UMLS2017AA full release
More [here](https://www.nlm.nih.gov/research/umls/index.html) on how to do that.

UMLS provides "entity types" named "semantic types".
The full list can be found [here](https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/documentation/SemanticTypesAndGroups.html).
We consider only:
- Chemical: T103 (Chmical), T120 (Chemical Viewed Functionally), T104 (Chemical Viewed Structurally), T197 (Inorganic Chemical), T109 (Organic Chemical)
- Disease: T047 (Disease or Syndrome), T050 (Experimental Model of Disease), T017 (Anatomical Structure), T033 (Finding), T038 (Biologic Function)

## Setup

This will generate all the data necessary to create the evaluation corpora.

```bash
# Don't forget to place the `umls-2017AA-full.zip` file in here!
export DIRECTORY="path/to/directory"
chmod +x ./joint_ner_nen/setup.sh
./joint_ner_nen/setup.sh $DIRECTORY
```

After this you can call the script that performs the mapping (UMLS->CTD, MONDO->CTD):

```bash
export DIRECTORY="path/to/directory"
(venv) user$ python -m joint_ner_nen.map_to_ctd --dir $DIRECTORY
```

## Evaluation

```bash
$ python -m nen.evaluate --gold ./data/eval/nen/gold --pred ./data/eval/nen/pred
```

Evaluate NEN performance given gold and prediction files in PubTator format

Each file in the `gold` folder and `pred/<method-subfolder>` should be named after the corpus from which it originates.

The directory structure should look like this one:

```
data/eval/nen
├── gold
│   └── bc5cdr.txt
└── pred
    └── pubtator
        └── bc5cdr.txt
```

We compute micro (average over entities) and macro (average over documents) precision, recall and f1 score,
both at the document and mention level.

### NOTE (mention level):

As gold mentions can have multiple identifiers.We use a **relaxed** version of a match,
i.e. we consider the prediction correct if any of the predicted identifier is equal to any of the gold ones.

## Multi-label

```bash
$ python -m nen.check_multi_label
```

Stats about amount of annoations w/ multiple identifiers. It prints them out to a TSV file for inspection.






